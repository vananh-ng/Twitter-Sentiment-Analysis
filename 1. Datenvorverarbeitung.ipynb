{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorverarbeitung\n",
    "Dieser Pythonskript entpricht Abschnitt 4.3 der Ausarbeitung.\n",
    "Bevor lernbasierte oder lexikalische Ansätze zur Sentimentanalyse angewendet werden können, müssen die Textdaten zunächst vorverarbeitet werden.\n",
    "Ziel ist es, dass das Textfeld nur Wörter enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten sind fertig verarbeitet\n"
     ]
    }
   ],
   "source": [
    "# notwendige Python-Pakete importieren\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# Datensatz vorbereiten\n",
    "cols = ['sentiment', 'id', 'date','query_string', 'user', 'text']\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",encoding = \"ISO-8859-1\",header=None, names=cols)\n",
    "df.drop(['id','date','query_string','user'],axis=1,inplace=True)\n",
    "df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "\n",
    "# Hier wird eine Methode zur Datenvorverarbeitung definiert\n",
    "# Zuerst muss eine Liste aller zusammengezogenen Wörter in der englischen Sprache erstellt\n",
    "con_dict = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \n",
    "            \"weren't\":\"were not\",\"haven't\":\"have not\",\"hasn't\":\"has not\",\n",
    "            \"hadn't\":\"had not\",\"won't\":\"will not\",\"wouldn't\":\"would not\",\n",
    "            \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "            \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\n",
    "            \"mightn't\":\"might not\",\"mustn't\":\"must not\", \"'s\":\" is\", \"'re\":\" are\", \n",
    "            \"'m\":\" am\", \"'ll\":\" will\", \"'d\":\" would\"}\n",
    "\n",
    "con_re = re.compile('(%s)' % '|'.join(con_dict.keys()))\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "# Hier findet die Datenvorverarbeitung statt.\n",
    "\n",
    "def dvv(text):    \n",
    "    wort_trennen = con_re.sub(lambda x: con_dict[x.group()],str(text))\n",
    "    url_entfernen = re.sub('((www.[^ ]+)|(https?://[^ ]+))','',wort_trennen)\n",
    "    soup = BeautifulSoup(url_entfernen, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    klein_umwandeln = str(souped).lower()\n",
    "    zahlen_entfern = re.sub(r'((@[A-Za-z0-9_]+)|([0-9]+)|([&?.!/;:%*+-<>§$]))', \" \", klein_umwandeln)\n",
    "    wort = [x for x  in tok.tokenize(zahlen_entfern) if len(x) > 1]\n",
    "    return (\" \".join(wort)).strip()\n",
    "\n",
    "# Alle Texte vom Datensatz werden verarbeitet...\n",
    "dvv_text = []\n",
    "for i in range(0,len(df)):\n",
    "    dvv_text.append(dvv(df['text'][i]))\n",
    "print('Daten sind fertig verarbeitet')\n",
    "\n",
    "#...und als eine .csv-Datei abgespeichert\n",
    "dvv_df = pd.DataFrame(dvv_text,columns=['text'])\n",
    "dvv_df.text.dropna(inplace=True)\n",
    "dvv_df['sentiment'] = df['sentiment']\n",
    "dvv_df.to_csv('clean_tweet.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Da es untersucht werden sollte, ob die Entfernung von Stoppwörter notwendig ist, \n",
    " werden zwei unterschiedliche verarbeitete Datensätze abgespeichert.\n",
    " Dieser enthält keine Stoppwörter.\n",
    "'''\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = stopwords.words('english')\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "dvv_df['text'] = dvv_df['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "dvv_df.to_csv('clean_tweet_sw.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
