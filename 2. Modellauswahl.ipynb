{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellauswahl\n",
    "In diesem Kapitel werden Experimente durchgeführt, um das beste Modell auszuwählen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Baseline\n",
    "Im ersten Test wird ein Baseline-Fall herangezogen, wobei keine Datenvorverarbeitung vorgenommen wird. Zur Feature Extraction wird das Bag-of-Words-Modell verwendet. Algorithmen wie die logistische Regression, die Support Vector Machines und das Naïve- Bayes-Modell werden für diesen Test ausgewertet. Das ausgewählte N-Gramme für die Baseline ist Unigramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Log8483ticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "cols = ['sentiment', 'id', 'date','query_string', 'user', 'text']\n",
    "df2 = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",encoding = \"ISO-8859-1\",header=None, names=cols)\n",
    "df2['sentiment'] = df2['sentiment'].map({0: 0, 4: 1})\n",
    "df2['text'].replace('', np.nan, inplace=True)\n",
    "\n",
    "x = df2.text\n",
    "y = df2.sentiment\n",
    "\n",
    "SEED = 2000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n",
      "Accuracy: 78.84%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7907    0.7878    0.7892     40237\n",
      "           1     0.7861    0.7889    0.7875     39763\n",
      "\n",
      "    accuracy                         0.7884     80000\n",
      "   macro avg     0.7884    0.7884    0.7884     80000\n",
      "weighted avg     0.7884    0.7884    0.7884     80000\n",
      "\n",
      "Anzahl von Features:  661318\n"
     ]
    }
   ],
   "source": [
    "# Zur Auswertung anderen Methoden können sie jeweils bei 'classifier' eingesetzt werden\n",
    "cvec = CountVectorizer()\n",
    "classifier = LinearSVC()\n",
    "\n",
    "# numerische Transformation durch BoW-Modell\n",
    "cvec.fit(x_train)\n",
    "x_train= cvec.transform(x_train)\n",
    "x_test = cvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "Accuracy: 78.20%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7644    0.8189    0.7907     40237\n",
      "           1     0.8025    0.7446    0.7725     39763\n",
      "\n",
      "    accuracy                         0.7820     80000\n",
      "   macro avg     0.7835    0.7818    0.7816     80000\n",
      "weighted avg     0.7834    0.7820    0.7817     80000\n",
      "\n",
      "Anzahl von Features:  661318\n"
     ]
    }
   ],
   "source": [
    "x = df2.text\n",
    "y = df2.sentiment\n",
    "\n",
    "SEED = 2000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)\n",
    "cvec = CountVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# numerische Transformation durch BoW-Modell\n",
    "cvec.fit(x_train)\n",
    "x_train= cvec.transform(x_train)\n",
    "x_test = cvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.1: Datenvorverarbeitung & BoW\n",
    "In diesem Test findet die Datenvorverarbeitung statt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ebenso werden verschiedene Methoden ausgewertet\n",
    "csv = 'clean_tweet_sw.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "x = my_df.text\n",
    "y = my_df.sentiment\n",
    "\n",
    "SEED = 2000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "Accuracy: 77.69%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7909    0.7531    0.7715     39806\n",
      "           1     0.7642    0.8008    0.7820     39782\n",
      "\n",
      "    accuracy                         0.7769     79588\n",
      "   macro avg     0.7775    0.7769    0.7768     79588\n",
      "weighted avg     0.7775    0.7769    0.7768     79588\n",
      "\n",
      "Anzahl von Features:  262511\n"
     ]
    }
   ],
   "source": [
    "cvec = CountVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "cvec.fit(x_train)\n",
    "x_train= cvec.transform(x_train)\n",
    "x_test = cvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n",
      "Accuracy: 77.03%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7834    0.7472    0.7649     39806\n",
      "           1     0.7583    0.7933    0.7754     39782\n",
      "\n",
      "    accuracy                         0.7703     79588\n",
      "   macro avg     0.7708    0.7703    0.7701     79588\n",
      "weighted avg     0.7708    0.7703    0.7701     79588\n",
      "\n",
      "Anzahl von Features:  262511\n"
     ]
    }
   ],
   "source": [
    "csv = 'clean_tweet_sw.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "SEED = 2000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)\n",
    "classifier = LinearSVC()\n",
    "cvec = CountVectorizer()\n",
    "# numerische Transformation durch BoW-Modell\n",
    "cvec.fit(x_train)\n",
    "x_train= cvec.transform(x_train)\n",
    "x_test = cvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "Accuracy: 76.74%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7633    0.7755    0.7693     39806\n",
      "           1     0.7717    0.7594    0.7655     39782\n",
      "\n",
      "    accuracy                         0.7674     79588\n",
      "   macro avg     0.7675    0.7674    0.7674     79588\n",
      "weighted avg     0.7675    0.7674    0.7674     79588\n",
      "\n",
      "Anzahl von Features:  262511\n"
     ]
    }
   ],
   "source": [
    "csv = 'clean_tweet_sw.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "SEED = 2000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)\n",
    "cvec = CountVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# numerische Transformation durch BoW-Modell\n",
    "cvec.fit(x_train)\n",
    "x_train= cvec.transform(x_train)\n",
    "x_test = cvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.2: Datenvorverarbeitung & TF-IDF\n",
    "Anstelle BoW-Modell wird die Auswirkung der TF-IDF-Methode untersucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = 'clean_tweet_sw.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "x = my_df.text\n",
    "y = my_df.sentiment\n",
    "\n",
    "SEED = 2000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "Accuracy: 77.92%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7902    0.7604    0.7750     39806\n",
      "           1     0.7690    0.7980    0.7832     39782\n",
      "\n",
      "    accuracy                         0.7792     79588\n",
      "   macro avg     0.7796    0.7792    0.7791     79588\n",
      "weighted avg     0.7796    0.7792    0.7791     79588\n",
      "\n",
      "Anzahl von Features:  262511\n"
     ]
    }
   ],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "tvec.fit(x_train)\n",
    "x_train= tvec.transform(x_train)\n",
    "x_test = tvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(tvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n",
      "Accuracy: 77.24%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7820    0.7554    0.7685     39806\n",
      "           1     0.7633    0.7894    0.7761     39782\n",
      "\n",
      "    accuracy                         0.7724     79588\n",
      "   macro avg     0.7727    0.7724    0.7723     79588\n",
      "weighted avg     0.7727    0.7724    0.7723     79588\n",
      "\n",
      "Anzahl von Features:  262511\n"
     ]
    }
   ],
   "source": [
    "csv = 'clean_tweet_sw.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "x = my_df.text\n",
    "y = my_df.sentiment\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)\n",
    "tvec = TfidfVectorizer()\n",
    "classifier = LinearSVC()\n",
    "tvec.fit(x_train)\n",
    "x_train= tvec.transform(x_train)\n",
    "x_test = tvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(tvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "Accuracy: 76.27%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7587    0.7706    0.7646     39806\n",
      "           1     0.7668    0.7547    0.7607     39782\n",
      "\n",
      "    accuracy                         0.7627     79588\n",
      "   macro avg     0.7628    0.7627    0.7627     79588\n",
      "weighted avg     0.7628    0.7627    0.7627     79588\n",
      "\n",
      "Anzahl von Features:  262511\n"
     ]
    }
   ],
   "source": [
    "csv = 'clean_tweet_sw.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "x = my_df.text\n",
    "y = my_df.sentiment\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "tvec.fit(x_train)\n",
    "x_train= tvec.transform(x_train)\n",
    "x_test = tvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(tvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Datenvorverarbeitung und TF-IDF (mit Stoppwörtern)\n",
    "Im Vergleich mit dem Testfall 1 – Baseline verschlechtern sich die Ergebnisse in Testfall 2.1 und Testfall 2.2, wenn eine Datenvorverarbeitung stattfindet. Daher wird es in diesem Test untersucht, ob Stoppwörter eine Auswirkung auf die Klassifizierungswerte haben. Bei der Datenvorverarbeitung werden zwei verschiedene Datein abgespeichert. Hier wird die Datei 'clean_tweet.csv' verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "Accuracy: 80.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8077    0.7881    0.7978     39951\n",
      "           1     0.7927    0.8120    0.8023     39877\n",
      "\n",
      "    accuracy                         0.8000     79828\n",
      "   macro avg     0.8002    0.8001    0.8000     79828\n",
      "weighted avg     0.8002    0.8000    0.8000     79828\n",
      "\n",
      "Anzahl von Features:  262636\n"
     ]
    }
   ],
   "source": [
    "# bereinigte Daten importieren und aufsplitten\n",
    "SEED = 26105111\n",
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "x = my_df.text\n",
    "y = my_df.sentiment\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "classifier = LogisticRegression()\n",
    "tvec.fit(x_train)\n",
    "x_train= tvec.transform(x_train)\n",
    "x_test = tvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(tvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n",
      "Accuracy: 79.35%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8004    0.7824    0.7913     39951\n",
      "           1     0.7868    0.8045    0.7956     39877\n",
      "\n",
      "    accuracy                         0.7935     79828\n",
      "   macro avg     0.7936    0.7935    0.7934     79828\n",
      "weighted avg     0.7936    0.7935    0.7934     79828\n",
      "\n",
      "Anzahl von Features:  262636\n"
     ]
    }
   ],
   "source": [
    "SEED = 26105111\n",
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "x = my_df.text\n",
    "y = my_df.sentiment\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "classifier = LinearSVC()\n",
    "tvec.fit(x_train)\n",
    "x_train= tvec.transform(x_train)\n",
    "x_test = tvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(tvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "Accuracy: 77.27%\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluationsmaße\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7640    0.7898    0.7767     39951\n",
      "           1     0.7821    0.7556    0.7686     39877\n",
      "\n",
      "    accuracy                         0.7727     79828\n",
      "   macro avg     0.7730    0.7727    0.7727     79828\n",
      "weighted avg     0.7730    0.7727    0.7727     79828\n",
      "\n",
      "Anzahl von Features:  262636\n"
     ]
    }
   ],
   "source": [
    "SEED = 26105111\n",
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "x = my_df.text\n",
    "y = my_df.sentiment\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "tvec.fit(x_train)\n",
    "x_train= tvec.transform(x_train)\n",
    "x_test = tvec.transform(x_test)\n",
    "sentiment_fit = classifier.fit(x_train, y_train)\n",
    "y_pred = sentiment_fit.predict(x_test)\n",
    "print(classifier)\n",
    "print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Evaluationsmaße\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print('Anzahl von Features: ', len(tvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Lexikalischer Ansatz (TextBlob)\n",
    "Der weitere Test prüft die Leistung der lexikalischen Methode in Bezug auf die vorgelegten\n",
    "Daten, wobei die sogenannte Methode TextBlob angewendet wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 61.06%\n",
      "--------------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "          predicted_positive  predicted_negative\n",
      "positive               35863                4029\n",
      "negative               27055               12881\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7617    0.3225    0.4532     39936\n",
      "           1     0.5700    0.8990    0.6977     39892\n",
      "\n",
      "    accuracy                         0.6106     79828\n",
      "   macro avg     0.6659    0.6108    0.5754     79828\n",
      "weighted avg     0.6659    0.6106    0.5754     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = my_df.text\n",
    "y = my_df.sentiment\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)\n",
    "\n",
    "from textblob import TextBlob\n",
    "tbresult = [TextBlob(i).sentiment.polarity for i in x_test] \n",
    "tbpred = [0 if n < 0 else 1 for n in tbresult]\n",
    "conmat = np.array(confusion_matrix(y_test, tbpred, labels= [1,0]))\n",
    "confusion = pd.DataFrame(conmat, index=['positive', 'negative'], columns=\n",
    "['predicted_positive','predicted_negative'])\n",
    "print(\"Accuracy Score: {0:.2f}%\".format(accuracy_score(y_test, tbpred)*100))\n",
    "print(\"-\"*80)\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion)\n",
    "print(\"-\"*80)\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_test, tbpred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testfall 5: Vergleich von Features und N-Gramme\n",
    "Im letzten Test wird die Auswirkung der N-Gramme sowie der Anzahl der Features auf die Klassifikatoren untersucht. Es werden nicht nur Unigramme, sondern auch Bigramme und Trigramme berücksichtigt. Für jeden Test werden Klassifikatoren (LR, SVM und NB) mit einer unterschiedlichen Anzahl von Features (zwischen 10 000 und 100 000 Features) trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bereinigte Daten importieren und aufsplitten\n",
    "# Beachten, dass der verwendeten Datensatz keine Stoppwörter hat\n",
    "SEED = 26105111\n",
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv)\n",
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "x = my_df.text\n",
    "y = my_df.sentiment\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.05, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N-Gramm: Unigramm, Bigramm oder Trigram\n",
    "- Anzahl der Features: 10000 bis 100000 Features\n",
    "- Algorithmen: Logitische Regression, SVM, NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary(classifier,vectorizer, x_train, y_train, x_test, y_test):\n",
    "    x_train = vectorizer.transform(x_train)\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    sentiment_fit = classifier.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    print(\"Accuracy: {0:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "    print(\"-\"*80)\n",
    "    print(\"Classification Report\\n\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logitische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_features = np.arange(10000,100001,10000)\n",
    "tvec = TfidfVectorizer()\n",
    "lr = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "\n",
    "def nfeature_accuracy_checker(vectorizer=tvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print (\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(max_features=n, ngram_range=ngram_range)\n",
    "        vectorizer.fit(x_train)\n",
    "        print(\"Ergebnisse bei {} Features\".format(n))\n",
    "        nfeature_accuracy = accuracy_summary(lr, tvec, x_train, y_train, x_test, y_test)\n",
    "        result.append((n,nfeature_accuracy))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2, max_iter=1000, n_jobs=-1)\n",
      "\n",
      "\n",
      "Ergebnisse bei 10000 Features\n",
      "Accuracy: 79.52%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8041    0.7811    0.7924     39951\n",
      "           1     0.7868    0.8094    0.7979     39877\n",
      "\n",
      "    accuracy                         0.7952     79828\n",
      "   macro avg     0.7954    0.7952    0.7952     79828\n",
      "weighted avg     0.7954    0.7952    0.7952     79828\n",
      "\n",
      "Ergebnisse bei 20000 Features\n",
      "Accuracy: 79.82%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8064    0.7852    0.7957     39951\n",
      "           1     0.7903    0.8112    0.8006     39877\n",
      "\n",
      "    accuracy                         0.7982     79828\n",
      "   macro avg     0.7984    0.7982    0.7981     79828\n",
      "weighted avg     0.7984    0.7982    0.7981     79828\n",
      "\n",
      "Ergebnisse bei 30000 Features\n",
      "Accuracy: 79.90%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8072    0.7862    0.7965     39951\n",
      "           1     0.7912    0.8118    0.8014     39877\n",
      "\n",
      "    accuracy                         0.7990     79828\n",
      "   macro avg     0.7992    0.7990    0.7990     79828\n",
      "weighted avg     0.7992    0.7990    0.7990     79828\n",
      "\n",
      "Ergebnisse bei 40000 Features\n",
      "Accuracy: 79.95%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8074    0.7870    0.7971     39951\n",
      "           1     0.7919    0.8120    0.8018     39877\n",
      "\n",
      "    accuracy                         0.7995     79828\n",
      "   macro avg     0.7997    0.7995    0.7994     79828\n",
      "weighted avg     0.7997    0.7995    0.7994     79828\n",
      "\n",
      "Ergebnisse bei 50000 Features\n",
      "Accuracy: 79.95%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8073    0.7872    0.7971     39951\n",
      "           1     0.7920    0.8118    0.8018     39877\n",
      "\n",
      "    accuracy                         0.7995     79828\n",
      "   macro avg     0.7997    0.7995    0.7995     79828\n",
      "weighted avg     0.7997    0.7995    0.7995     79828\n",
      "\n",
      "Ergebnisse bei 60000 Features\n",
      "Accuracy: 79.93%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8072    0.7869    0.7969     39951\n",
      "           1     0.7918    0.8116    0.8016     39877\n",
      "\n",
      "    accuracy                         0.7993     79828\n",
      "   macro avg     0.7995    0.7993    0.7993     79828\n",
      "weighted avg     0.7995    0.7993    0.7993     79828\n",
      "\n",
      "Ergebnisse bei 70000 Features\n",
      "Accuracy: 79.93%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8070    0.7871    0.7969     39951\n",
      "           1     0.7919    0.8115    0.8015     39877\n",
      "\n",
      "    accuracy                         0.7993     79828\n",
      "   macro avg     0.7994    0.7993    0.7992     79828\n",
      "weighted avg     0.7995    0.7993    0.7992     79828\n",
      "\n",
      "Ergebnisse bei 80000 Features\n",
      "Accuracy: 79.92%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8070    0.7870    0.7969     39951\n",
      "           1     0.7918    0.8115    0.8015     39877\n",
      "\n",
      "    accuracy                         0.7992     79828\n",
      "   macro avg     0.7994    0.7992    0.7992     79828\n",
      "weighted avg     0.7994    0.7992    0.7992     79828\n",
      "\n",
      "Ergebnisse bei 90000 Features\n",
      "Accuracy: 79.92%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8069    0.7870    0.7968     39951\n",
      "           1     0.7918    0.8113    0.8014     39877\n",
      "\n",
      "    accuracy                         0.7992     79828\n",
      "   macro avg     0.7993    0.7992    0.7991     79828\n",
      "weighted avg     0.7994    0.7992    0.7991     79828\n",
      "\n",
      "Ergebnisse bei 100000 Features\n",
      "Accuracy: 79.90%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8069    0.7866    0.7967     39951\n",
      "           1     0.7915    0.8114    0.8013     39877\n",
      "\n",
      "    accuracy                         0.7990     79828\n",
      "   macro avg     0.7992    0.7990    0.7990     79828\n",
      "weighted avg     0.7992    0.7990    0.7990     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_result_1gram = nfeature_accuracy_checker(vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2, max_iter=1000, n_jobs=-1)\n",
      "\n",
      "\n",
      "Ergebnisse bei 10000 Features\n",
      "Accuracy: 80.56%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8133    0.7937    0.8034     39951\n",
      "           1     0.7982    0.8174    0.8077     39877\n",
      "\n",
      "    accuracy                         0.8056     79828\n",
      "   macro avg     0.8058    0.8056    0.8056     79828\n",
      "weighted avg     0.8058    0.8056    0.8056     79828\n",
      "\n",
      "Ergebnisse bei 20000 Features\n",
      "Accuracy: 81.35%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8213    0.8016    0.8114     39951\n",
      "           1     0.8059    0.8253    0.8155     39877\n",
      "\n",
      "    accuracy                         0.8135     79828\n",
      "   macro avg     0.8136    0.8135    0.8134     79828\n",
      "weighted avg     0.8136    0.8135    0.8134     79828\n",
      "\n",
      "Ergebnisse bei 30000 Features\n",
      "Accuracy: 81.56%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8236    0.8037    0.8135     39951\n",
      "           1     0.8080    0.8275    0.8176     39877\n",
      "\n",
      "    accuracy                         0.8156     79828\n",
      "   macro avg     0.8158    0.8156    0.8156     79828\n",
      "weighted avg     0.8158    0.8156    0.8156     79828\n",
      "\n",
      "Ergebnisse bei 40000 Features\n",
      "Accuracy: 81.83%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8265    0.8062    0.8162     39951\n",
      "           1     0.8105    0.8305    0.8204     39877\n",
      "\n",
      "    accuracy                         0.8183     79828\n",
      "   macro avg     0.8185    0.8183    0.8183     79828\n",
      "weighted avg     0.8185    0.8183    0.8183     79828\n",
      "\n",
      "Ergebnisse bei 50000 Features\n",
      "Accuracy: 82.02%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8283    0.8082    0.8181     39951\n",
      "           1     0.8124    0.8321    0.8221     39877\n",
      "\n",
      "    accuracy                         0.8202     79828\n",
      "   macro avg     0.8203    0.8202    0.8201     79828\n",
      "weighted avg     0.8203    0.8202    0.8201     79828\n",
      "\n",
      "Ergebnisse bei 60000 Features\n",
      "Accuracy: 82.01%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8279    0.8085    0.8181     39951\n",
      "           1     0.8126    0.8316    0.8220     39877\n",
      "\n",
      "    accuracy                         0.8201     79828\n",
      "   macro avg     0.8202    0.8201    0.8200     79828\n",
      "weighted avg     0.8202    0.8201    0.8200     79828\n",
      "\n",
      "Ergebnisse bei 70000 Features\n",
      "Accuracy: 82.14%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8297    0.8092    0.8193     39951\n",
      "           1     0.8135    0.8336    0.8234     39877\n",
      "\n",
      "    accuracy                         0.8214     79828\n",
      "   macro avg     0.8216    0.8214    0.8214     79828\n",
      "weighted avg     0.8216    0.8214    0.8214     79828\n",
      "\n",
      "Ergebnisse bei 80000 Features\n",
      "Accuracy: 82.14%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8292    0.8100    0.8195     39951\n",
      "           1     0.8140    0.8329    0.8233     39877\n",
      "\n",
      "    accuracy                         0.8214     79828\n",
      "   macro avg     0.8216    0.8214    0.8214     79828\n",
      "weighted avg     0.8216    0.8214    0.8214     79828\n",
      "\n",
      "Ergebnisse bei 90000 Features\n",
      "Accuracy: 82.16%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8295    0.8100    0.8196     39951\n",
      "           1     0.8140    0.8332    0.8235     39877\n",
      "\n",
      "    accuracy                         0.8216     79828\n",
      "   macro avg     0.8218    0.8216    0.8216     79828\n",
      "weighted avg     0.8218    0.8216    0.8216     79828\n",
      "\n",
      "Ergebnisse bei 100000 Features\n",
      "Accuracy: 82.21%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8298    0.8108    0.8202     39951\n",
      "           1     0.8147    0.8334    0.8239     39877\n",
      "\n",
      "    accuracy                         0.8221     79828\n",
      "   macro avg     0.8223    0.8221    0.8221     79828\n",
      "weighted avg     0.8223    0.8221    0.8221     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_result_2gram = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2, max_iter=1000, n_jobs=-1)\n",
      "\n",
      "\n",
      "Ergebnisse bei 10000 Features\n",
      "Accuracy: 80.52%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8134    0.7925    0.8028     39951\n",
      "           1     0.7973    0.8178    0.8075     39877\n",
      "\n",
      "    accuracy                         0.8052     79828\n",
      "   macro avg     0.8054    0.8052    0.8051     79828\n",
      "weighted avg     0.8054    0.8052    0.8051     79828\n",
      "\n",
      "Ergebnisse bei 20000 Features\n",
      "Accuracy: 81.33%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8221    0.8002    0.8110     39951\n",
      "           1     0.8050    0.8265    0.8156     39877\n",
      "\n",
      "    accuracy                         0.8133     79828\n",
      "   macro avg     0.8136    0.8133    0.8133     79828\n",
      "weighted avg     0.8136    0.8133    0.8133     79828\n",
      "\n",
      "Ergebnisse bei 30000 Features\n",
      "Accuracy: 81.62%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8247    0.8036    0.8140     39951\n",
      "           1     0.8081    0.8288    0.8183     39877\n",
      "\n",
      "    accuracy                         0.8162     79828\n",
      "   macro avg     0.8164    0.8162    0.8162     79828\n",
      "weighted avg     0.8164    0.8162    0.8162     79828\n",
      "\n",
      "Ergebnisse bei 40000 Features\n",
      "Accuracy: 81.88%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8266    0.8072    0.8168     39951\n",
      "           1     0.8113    0.8304    0.8207     39877\n",
      "\n",
      "    accuracy                         0.8188     79828\n",
      "   macro avg     0.8189    0.8188    0.8187     79828\n",
      "weighted avg     0.8189    0.8188    0.8187     79828\n",
      "\n",
      "Ergebnisse bei 50000 Features\n",
      "Accuracy: 82.03%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8275    0.8096    0.8185     39951\n",
      "           1     0.8133    0.8309    0.8220     39877\n",
      "\n",
      "    accuracy                         0.8203     79828\n",
      "   macro avg     0.8204    0.8203    0.8202     79828\n",
      "weighted avg     0.8204    0.8203    0.8202     79828\n",
      "\n",
      "Ergebnisse bei 60000 Features\n",
      "Accuracy: 82.14%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8294    0.8096    0.8194     39951\n",
      "           1     0.8137    0.8331    0.8233     39877\n",
      "\n",
      "    accuracy                         0.8214     79828\n",
      "   macro avg     0.8215    0.8214    0.8213     79828\n",
      "weighted avg     0.8215    0.8214    0.8213     79828\n",
      "\n",
      "Ergebnisse bei 70000 Features\n",
      "Accuracy: 82.27%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8307    0.8111    0.8208     39951\n",
      "           1     0.8151    0.8344    0.8247     39877\n",
      "\n",
      "    accuracy                         0.8227     79828\n",
      "   macro avg     0.8229    0.8228    0.8227     79828\n",
      "weighted avg     0.8229    0.8227    0.8227     79828\n",
      "\n",
      "Ergebnisse bei 80000 Features\n",
      "Accuracy: 82.29%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8307    0.8115    0.8210     39951\n",
      "           1     0.8154    0.8343    0.8248     39877\n",
      "\n",
      "    accuracy                         0.8229     79828\n",
      "   macro avg     0.8231    0.8229    0.8229     79828\n",
      "weighted avg     0.8231    0.8229    0.8229     79828\n",
      "\n",
      "Ergebnisse bei 90000 Features\n",
      "Accuracy: 82.34%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8315    0.8117    0.8215     39951\n",
      "           1     0.8157    0.8352    0.8254     39877\n",
      "\n",
      "    accuracy                         0.8234     79828\n",
      "   macro avg     0.8236    0.8234    0.8234     79828\n",
      "weighted avg     0.8236    0.8234    0.8234     79828\n",
      "\n",
      "Ergebnisse bei 100000 Features\n",
      "Accuracy: 82.41%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8322    0.8122    0.8221     39951\n",
      "           1     0.8163    0.8360    0.8260     39877\n",
      "\n",
      "    accuracy                         0.8241     79828\n",
      "   macro avg     0.8243    0.8241    0.8240     79828\n",
      "weighted avg     0.8243    0.8241    0.8240     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_result_3gram = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "n_features = np.arange(10000,100001,10000)\n",
    "tvec = TfidfVectorizer()\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfeature_accuracy_checker(vectorizer=tvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=svm):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print (\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(max_features=n, ngram_range=ngram_range)\n",
    "        vectorizer.fit(x_train)\n",
    "        print(\"Ergebnisse bei {} Features\".format(n))\n",
    "        nfeature_accuracy = accuracy_summary(svm, tvec, x_train, y_train, x_test, y_test)\n",
    "        result.append((n,nfeature_accuracy))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n",
      "\n",
      "\n",
      "Ergebnisse bei 10000 Features\n",
      "Accuracy: 79.50%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8057    0.7780    0.7916     39951\n",
      "           1     0.7850    0.8120    0.7983     39877\n",
      "\n",
      "    accuracy                         0.7950     79828\n",
      "   macro avg     0.7953    0.7950    0.7949     79828\n",
      "weighted avg     0.7953    0.7950    0.7949     79828\n",
      "\n",
      "Ergebnisse bei 20000 Features\n",
      "Accuracy: 79.71%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8070    0.7816    0.7941     39951\n",
      "           1     0.7879    0.8127    0.8001     39877\n",
      "\n",
      "    accuracy                         0.7971     79828\n",
      "   macro avg     0.7974    0.7971    0.7971     79828\n",
      "weighted avg     0.7974    0.7971    0.7971     79828\n",
      "\n",
      "Ergebnisse bei 30000 Features\n",
      "Accuracy: 79.70%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8063    0.7823    0.7941     39951\n",
      "           1     0.7882    0.8117    0.7998     39877\n",
      "\n",
      "    accuracy                         0.7970     79828\n",
      "   macro avg     0.7972    0.7970    0.7969     79828\n",
      "weighted avg     0.7973    0.7970    0.7969     79828\n",
      "\n",
      "Ergebnisse bei 40000 Features\n",
      "Accuracy: 79.69%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8060    0.7826    0.7941     39951\n",
      "           1     0.7883    0.8113    0.7996     39877\n",
      "\n",
      "    accuracy                         0.7969     79828\n",
      "   macro avg     0.7972    0.7969    0.7969     79828\n",
      "weighted avg     0.7972    0.7969    0.7969     79828\n",
      "\n",
      "Ergebnisse bei 50000 Features\n",
      "Accuracy: 79.60%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8048    0.7821    0.7933     39951\n",
      "           1     0.7877    0.8099    0.7987     39877\n",
      "\n",
      "    accuracy                         0.7960     79828\n",
      "   macro avg     0.7962    0.7960    0.7960     79828\n",
      "weighted avg     0.7962    0.7960    0.7960     79828\n",
      "\n",
      "Ergebnisse bei 60000 Features\n",
      "Accuracy: 79.55%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8038    0.7824    0.7930     39951\n",
      "           1     0.7877    0.8086    0.7980     39877\n",
      "\n",
      "    accuracy                         0.7955     79828\n",
      "   macro avg     0.7957    0.7955    0.7955     79828\n",
      "weighted avg     0.7957    0.7955    0.7955     79828\n",
      "\n",
      "Ergebnisse bei 70000 Features\n",
      "Accuracy: 79.53%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8035    0.7821    0.7927     39951\n",
      "           1     0.7874    0.8084    0.7978     39877\n",
      "\n",
      "    accuracy                         0.7953     79828\n",
      "   macro avg     0.7955    0.7953    0.7952     79828\n",
      "weighted avg     0.7955    0.7953    0.7952     79828\n",
      "\n",
      "Ergebnisse bei 80000 Features\n",
      "Accuracy: 79.51%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8034    0.7819    0.7925     39951\n",
      "           1     0.7872    0.8083    0.7976     39877\n",
      "\n",
      "    accuracy                         0.7951     79828\n",
      "   macro avg     0.7953    0.7951    0.7951     79828\n",
      "weighted avg     0.7953    0.7951    0.7951     79828\n",
      "\n",
      "Ergebnisse bei 90000 Features\n",
      "Accuracy: 79.48%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8028    0.7820    0.7923     39951\n",
      "           1     0.7871    0.8076    0.7972     39877\n",
      "\n",
      "    accuracy                         0.7948     79828\n",
      "   macro avg     0.7950    0.7948    0.7947     79828\n",
      "weighted avg     0.7950    0.7948    0.7947     79828\n",
      "\n",
      "Ergebnisse bei 100000 Features\n",
      "Accuracy: 79.43%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8025    0.7814    0.7918     39951\n",
      "           1     0.7866    0.8073    0.7968     39877\n",
      "\n",
      "    accuracy                         0.7943     79828\n",
      "   macro avg     0.7945    0.7943    0.7943     79828\n",
      "weighted avg     0.7945    0.7943    0.7943     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_result_1gram = nfeature_accuracy_checker(vectorizer=tvec, classifier=svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n",
      "\n",
      "\n",
      "Ergebnisse bei 10000 Features\n",
      "Accuracy: 80.48%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8148    0.7894    0.8019     39951\n",
      "           1     0.7954    0.8203    0.8076     39877\n",
      "\n",
      "    accuracy                         0.8048     79828\n",
      "   macro avg     0.8051    0.8048    0.8048     79828\n",
      "weighted avg     0.8051    0.8048    0.8048     79828\n",
      "\n",
      "Ergebnisse bei 20000 Features\n",
      "Accuracy: 81.29%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8233    0.7973    0.8101     39951\n",
      "           1     0.8032    0.8285    0.8156     39877\n",
      "\n",
      "    accuracy                         0.8129     79828\n",
      "   macro avg     0.8132    0.8129    0.8129     79828\n",
      "weighted avg     0.8132    0.8129    0.8129     79828\n",
      "\n",
      "Ergebnisse bei 30000 Features\n",
      "Accuracy: 81.48%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8247    0.8001    0.8122     39951\n",
      "           1     0.8055    0.8296    0.8174     39877\n",
      "\n",
      "    accuracy                         0.8148     79828\n",
      "   macro avg     0.8151    0.8148    0.8148     79828\n",
      "weighted avg     0.8151    0.8148    0.8148     79828\n",
      "\n",
      "Ergebnisse bei 40000 Features\n",
      "Accuracy: 81.71%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8266    0.8031    0.8147     39951\n",
      "           1     0.8082    0.8312    0.8195     39877\n",
      "\n",
      "    accuracy                         0.8171     79828\n",
      "   macro avg     0.8174    0.8172    0.8171     79828\n",
      "weighted avg     0.8174    0.8171    0.8171     79828\n",
      "\n",
      "Ergebnisse bei 50000 Features\n",
      "Accuracy: 81.84%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8279    0.8045    0.8160     39951\n",
      "           1     0.8095    0.8324    0.8208     39877\n",
      "\n",
      "    accuracy                         0.8184     79828\n",
      "   macro avg     0.8187    0.8185    0.8184     79828\n",
      "weighted avg     0.8187    0.8184    0.8184     79828\n",
      "\n",
      "Ergebnisse bei 60000 Features\n",
      "Accuracy: 81.78%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8268    0.8043    0.8154     39951\n",
      "           1     0.8092    0.8312    0.8201     39877\n",
      "\n",
      "    accuracy                         0.8178     79828\n",
      "   macro avg     0.8180    0.8178    0.8177     79828\n",
      "weighted avg     0.8180    0.8178    0.8177     79828\n",
      "\n",
      "Ergebnisse bei 70000 Features\n",
      "Accuracy: 81.87%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8278    0.8053    0.8164     39951\n",
      "           1     0.8101    0.8322    0.8210     39877\n",
      "\n",
      "    accuracy                         0.8187     79828\n",
      "   macro avg     0.8190    0.8187    0.8187     79828\n",
      "weighted avg     0.8190    0.8187    0.8187     79828\n",
      "\n",
      "Ergebnisse bei 80000 Features\n",
      "Accuracy: 81.84%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8273    0.8054    0.8162     39951\n",
      "           1     0.8100    0.8315    0.8206     39877\n",
      "\n",
      "    accuracy                         0.8184     79828\n",
      "   macro avg     0.8187    0.8184    0.8184     79828\n",
      "weighted avg     0.8187    0.8184    0.8184     79828\n",
      "\n",
      "Ergebnisse bei 90000 Features\n",
      "Accuracy: 81.79%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8267    0.8047    0.8156     39951\n",
      "           1     0.8094    0.8311    0.8201     39877\n",
      "\n",
      "    accuracy                         0.8179     79828\n",
      "   macro avg     0.8181    0.8179    0.8178     79828\n",
      "weighted avg     0.8181    0.8179    0.8178     79828\n",
      "\n",
      "Ergebnisse bei 100000 Features\n",
      "Accuracy: 81.80%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8263    0.8057    0.8159     39951\n",
      "           1     0.8101    0.8303    0.8201     39877\n",
      "\n",
      "    accuracy                         0.8180     79828\n",
      "   macro avg     0.8182    0.8180    0.8180     79828\n",
      "weighted avg     0.8182    0.8180    0.8180     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_result_2gram = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n",
      "\n",
      "\n",
      "Ergebnisse bei 10000 Features\n",
      "Accuracy: 80.37%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8138    0.7881    0.8008     39951\n",
      "           1     0.7942    0.8194    0.8066     39877\n",
      "\n",
      "    accuracy                         0.8037     79828\n",
      "   macro avg     0.8040    0.8038    0.8037     79828\n",
      "weighted avg     0.8041    0.8037    0.8037     79828\n",
      "\n",
      "Ergebnisse bei 20000 Features\n",
      "Accuracy: 81.21%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8230    0.7956    0.8091     39951\n",
      "           1     0.8018    0.8286    0.8150     39877\n",
      "\n",
      "    accuracy                         0.8121     79828\n",
      "   macro avg     0.8124    0.8121    0.8120     79828\n",
      "weighted avg     0.8124    0.8121    0.8120     79828\n",
      "\n",
      "Ergebnisse bei 30000 Features\n",
      "Accuracy: 81.55%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8262    0.7996    0.8127     39951\n",
      "           1     0.8055    0.8315    0.8183     39877\n",
      "\n",
      "    accuracy                         0.8155     79828\n",
      "   macro avg     0.8159    0.8156    0.8155     79828\n",
      "weighted avg     0.8159    0.8155    0.8155     79828\n",
      "\n",
      "Ergebnisse bei 40000 Features\n",
      "Accuracy: 81.74%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8275    0.8025    0.8148     39951\n",
      "           1     0.8079    0.8324    0.8200     39877\n",
      "\n",
      "    accuracy                         0.8174     79828\n",
      "   macro avg     0.8177    0.8174    0.8174     79828\n",
      "weighted avg     0.8177    0.8174    0.8174     79828\n",
      "\n",
      "Ergebnisse bei 50000 Features\n",
      "Accuracy: 81.87%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8277    0.8052    0.8163     39951\n",
      "           1     0.8100    0.8321    0.8209     39877\n",
      "\n",
      "    accuracy                         0.8187     79828\n",
      "   macro avg     0.8189    0.8187    0.8186     79828\n",
      "weighted avg     0.8189    0.8187    0.8186     79828\n",
      "\n",
      "Ergebnisse bei 60000 Features\n",
      "Accuracy: 81.96%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8289    0.8059    0.8172     39951\n",
      "           1     0.8108    0.8333    0.8219     39877\n",
      "\n",
      "    accuracy                         0.8196     79828\n",
      "   macro avg     0.8198    0.8196    0.8196     79828\n",
      "weighted avg     0.8198    0.8196    0.8196     79828\n",
      "\n",
      "Ergebnisse bei 70000 Features\n",
      "Accuracy: 82.02%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8292    0.8069    0.8179     39951\n",
      "           1     0.8116    0.8335    0.8224     39877\n",
      "\n",
      "    accuracy                         0.8202     79828\n",
      "   macro avg     0.8204    0.8202    0.8202     79828\n",
      "weighted avg     0.8204    0.8202    0.8202     79828\n",
      "\n",
      "Ergebnisse bei 80000 Features\n",
      "Accuracy: 81.96%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8288    0.8061    0.8173     39951\n",
      "           1     0.8109    0.8332    0.8219     39877\n",
      "\n",
      "    accuracy                         0.8196     79828\n",
      "   macro avg     0.8199    0.8196    0.8196     79828\n",
      "weighted avg     0.8199    0.8196    0.8196     79828\n",
      "\n",
      "Ergebnisse bei 90000 Features\n",
      "Accuracy: 81.95%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8288    0.8059    0.8172     39951\n",
      "           1     0.8108    0.8332    0.8218     39877\n",
      "\n",
      "    accuracy                         0.8195     79828\n",
      "   macro avg     0.8198    0.8195    0.8195     79828\n",
      "weighted avg     0.8198    0.8195    0.8195     79828\n",
      "\n",
      "Ergebnisse bei 100000 Features\n",
      "Accuracy: 81.95%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8288    0.8057    0.8171     39951\n",
      "           1     0.8106    0.8332    0.8218     39877\n",
      "\n",
      "    accuracy                         0.8195     79828\n",
      "   macro avg     0.8197    0.8195    0.8194     79828\n",
      "weighted avg     0.8197    0.8195    0.8194     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_result_3gram = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Naive Bayes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "multiNB = MultinomialNB()\n",
    "\n",
    "def nfeature_accuracy_checker(vectorizer=tvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=multiNB):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print (\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(max_features=n, ngram_range=ngram_range)\n",
    "        vectorizer.fit(x_train)\n",
    "        print(\"Ergebnisse bei {} Features\".format(n))\n",
    "        nfeature_accuracy = accuracy_summary(multiNB, tvec, x_train, y_train, x_test, y_test)\n",
    "        result.append((n,nfeature_accuracy))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "\n",
      "\n",
      "Ergebnisse bei 10000 Features\n",
      "Accuracy: 77.10%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7697    0.7741    0.7719     39951\n",
      "           1     0.7724    0.7679    0.7701     39877\n",
      "\n",
      "    accuracy                         0.7710     79828\n",
      "   macro avg     0.7710    0.7710    0.7710     79828\n",
      "weighted avg     0.7710    0.7710    0.7710     79828\n",
      "\n",
      "Ergebnisse bei 20000 Features\n",
      "Accuracy: 77.37%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7717    0.7779    0.7748     39951\n",
      "           1     0.7757    0.7694    0.7725     39877\n",
      "\n",
      "    accuracy                         0.7737     79828\n",
      "   macro avg     0.7737    0.7737    0.7737     79828\n",
      "weighted avg     0.7737    0.7737    0.7737     79828\n",
      "\n",
      "Ergebnisse bei 30000 Features\n",
      "Accuracy: 77.40%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7715    0.7792    0.7753     39951\n",
      "           1     0.7766    0.7688    0.7726     39877\n",
      "\n",
      "    accuracy                         0.7740     79828\n",
      "   macro avg     0.7740    0.7740    0.7740     79828\n",
      "weighted avg     0.7740    0.7740    0.7740     79828\n",
      "\n",
      "Ergebnisse bei 40000 Features\n",
      "Accuracy: 77.38%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7707    0.7801    0.7754     39951\n",
      "           1     0.7770    0.7674    0.7722     39877\n",
      "\n",
      "    accuracy                         0.7738     79828\n",
      "   macro avg     0.7738    0.7738    0.7738     79828\n",
      "weighted avg     0.7738    0.7738    0.7738     79828\n",
      "\n",
      "Ergebnisse bei 50000 Features\n",
      "Accuracy: 77.36%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7701    0.7805    0.7753     39951\n",
      "           1     0.7771    0.7666    0.7718     39877\n",
      "\n",
      "    accuracy                         0.7736     79828\n",
      "   macro avg     0.7736    0.7736    0.7735     79828\n",
      "weighted avg     0.7736    0.7736    0.7736     79828\n",
      "\n",
      "Ergebnisse bei 60000 Features\n",
      "Accuracy: 77.33%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7695    0.7810    0.7752     39951\n",
      "           1     0.7772    0.7656    0.7714     39877\n",
      "\n",
      "    accuracy                         0.7733     79828\n",
      "   macro avg     0.7733    0.7733    0.7733     79828\n",
      "weighted avg     0.7733    0.7733    0.7733     79828\n",
      "\n",
      "Ergebnisse bei 70000 Features\n",
      "Accuracy: 77.35%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7693    0.7819    0.7755     39951\n",
      "           1     0.7778    0.7651    0.7714     39877\n",
      "\n",
      "    accuracy                         0.7735     79828\n",
      "   macro avg     0.7736    0.7735    0.7735     79828\n",
      "weighted avg     0.7736    0.7735    0.7735     79828\n",
      "\n",
      "Ergebnisse bei 80000 Features\n",
      "Accuracy: 77.34%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7690    0.7823    0.7756     39951\n",
      "           1     0.7780    0.7646    0.7712     39877\n",
      "\n",
      "    accuracy                         0.7734     79828\n",
      "   macro avg     0.7735    0.7734    0.7734     79828\n",
      "weighted avg     0.7735    0.7734    0.7734     79828\n",
      "\n",
      "Ergebnisse bei 90000 Features\n",
      "Accuracy: 77.33%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7686    0.7826    0.7755     39951\n",
      "           1     0.7782    0.7639    0.7709     39877\n",
      "\n",
      "    accuracy                         0.7733     79828\n",
      "   macro avg     0.7734    0.7733    0.7732     79828\n",
      "weighted avg     0.7734    0.7733    0.7732     79828\n",
      "\n",
      "Ergebnisse bei 100000 Features\n",
      "Accuracy: 77.32%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7681    0.7833    0.7756     39951\n",
      "           1     0.7785    0.7631    0.7707     39877\n",
      "\n",
      "    accuracy                         0.7732     79828\n",
      "   macro avg     0.7733    0.7732    0.7732     79828\n",
      "weighted avg     0.7733    0.7732    0.7732     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_result_1gram = nfeature_accuracy_checker(vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "\n",
      "\n",
      "Ergebnisse bei 10000 Features\n",
      "Accuracy: 78.23%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7794    0.7879    0.7836     39951\n",
      "           1     0.7852    0.7766    0.7809     39877\n",
      "\n",
      "    accuracy                         0.7823     79828\n",
      "   macro avg     0.7823    0.7823    0.7822     79828\n",
      "weighted avg     0.7823    0.7823    0.7822     79828\n",
      "\n",
      "Ergebnisse bei 20000 Features\n",
      "Accuracy: 78.94%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7864    0.7951    0.7907     39951\n",
      "           1     0.7924    0.7836    0.7880     39877\n",
      "\n",
      "    accuracy                         0.7894     79828\n",
      "   macro avg     0.7894    0.7894    0.7894     79828\n",
      "weighted avg     0.7894    0.7894    0.7894     79828\n",
      "\n",
      "Ergebnisse bei 30000 Features\n",
      "Accuracy: 79.19%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7893    0.7969    0.7931     39951\n",
      "           1     0.7946    0.7868    0.7907     39877\n",
      "\n",
      "    accuracy                         0.7919     79828\n",
      "   macro avg     0.7919    0.7919    0.7919     79828\n",
      "weighted avg     0.7919    0.7919    0.7919     79828\n",
      "\n",
      "Ergebnisse bei 40000 Features\n",
      "Accuracy: 79.40%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7911    0.7994    0.7952     39951\n",
      "           1     0.7969    0.7886    0.7927     39877\n",
      "\n",
      "    accuracy                         0.7940     79828\n",
      "   macro avg     0.7940    0.7940    0.7940     79828\n",
      "weighted avg     0.7940    0.7940    0.7940     79828\n",
      "\n",
      "Ergebnisse bei 50000 Features\n",
      "Accuracy: 79.58%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7932    0.8008    0.7970     39951\n",
      "           1     0.7985    0.7908    0.7946     39877\n",
      "\n",
      "    accuracy                         0.7958     79828\n",
      "   macro avg     0.7958    0.7958    0.7958     79828\n",
      "weighted avg     0.7958    0.7958    0.7958     79828\n",
      "\n",
      "Ergebnisse bei 60000 Features\n",
      "Accuracy: 79.65%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7943    0.8008    0.7975     39951\n",
      "           1     0.7988    0.7922    0.7955     39877\n",
      "\n",
      "    accuracy                         0.7965     79828\n",
      "   macro avg     0.7965    0.7965    0.7965     79828\n",
      "weighted avg     0.7965    0.7965    0.7965     79828\n",
      "\n",
      "Ergebnisse bei 70000 Features\n",
      "Accuracy: 79.70%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7954    0.8003    0.7978     39951\n",
      "           1     0.7987    0.7937    0.7962     39877\n",
      "\n",
      "    accuracy                         0.7970     79828\n",
      "   macro avg     0.7970    0.7970    0.7970     79828\n",
      "weighted avg     0.7970    0.7970    0.7970     79828\n",
      "\n",
      "Ergebnisse bei 80000 Features\n",
      "Accuracy: 79.80%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7962    0.8014    0.7988     39951\n",
      "           1     0.7997    0.7945    0.7971     39877\n",
      "\n",
      "    accuracy                         0.7980     79828\n",
      "   macro avg     0.7980    0.7979    0.7979     79828\n",
      "weighted avg     0.7980    0.7980    0.7980     79828\n",
      "\n",
      "Ergebnisse bei 90000 Features\n",
      "Accuracy: 79.87%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7969    0.8023    0.7996     39951\n",
      "           1     0.8006    0.7951    0.7979     39877\n",
      "\n",
      "    accuracy                         0.7987     79828\n",
      "   macro avg     0.7988    0.7987    0.7987     79828\n",
      "weighted avg     0.7988    0.7987    0.7987     79828\n",
      "\n",
      "Ergebnisse bei 100000 Features\n",
      "Accuracy: 79.94%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7974    0.8033    0.8003     39951\n",
      "           1     0.8015    0.7955    0.7985     39877\n",
      "\n",
      "    accuracy                         0.7994     79828\n",
      "   macro avg     0.7994    0.7994    0.7994     79828\n",
      "weighted avg     0.7994    0.7994    0.7994     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_result_2gram = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "\n",
      "\n",
      "Ergebnisse bei 10000 Features\n",
      "Accuracy: 78.01%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7776    0.7850    0.7813     39951\n",
      "           1     0.7826    0.7751    0.7788     39877\n",
      "\n",
      "    accuracy                         0.7801     79828\n",
      "   macro avg     0.7801    0.7801    0.7801     79828\n",
      "weighted avg     0.7801    0.7801    0.7801     79828\n",
      "\n",
      "Ergebnisse bei 20000 Features\n",
      "Accuracy: 78.87%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7863    0.7933    0.7898     39951\n",
      "           1     0.7911    0.7840    0.7875     39877\n",
      "\n",
      "    accuracy                         0.7887     79828\n",
      "   macro avg     0.7887    0.7887    0.7887     79828\n",
      "weighted avg     0.7887    0.7887    0.7887     79828\n",
      "\n",
      "Ergebnisse bei 30000 Features\n",
      "Accuracy: 79.19%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7906    0.7948    0.7927     39951\n",
      "           1     0.7933    0.7890    0.7912     39877\n",
      "\n",
      "    accuracy                         0.7919     79828\n",
      "   macro avg     0.7919    0.7919    0.7919     79828\n",
      "weighted avg     0.7919    0.7919    0.7919     79828\n",
      "\n",
      "Ergebnisse bei 40000 Features\n",
      "Accuracy: 79.45%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7929    0.7977    0.7953     39951\n",
      "           1     0.7961    0.7912    0.7936     39877\n",
      "\n",
      "    accuracy                         0.7945     79828\n",
      "   macro avg     0.7945    0.7945    0.7945     79828\n",
      "weighted avg     0.7945    0.7945    0.7945     79828\n",
      "\n",
      "Ergebnisse bei 50000 Features\n",
      "Accuracy: 79.61%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7945    0.7994    0.7970     39951\n",
      "           1     0.7978    0.7929    0.7953     39877\n",
      "\n",
      "    accuracy                         0.7961     79828\n",
      "   macro avg     0.7961    0.7961    0.7961     79828\n",
      "weighted avg     0.7961    0.7961    0.7961     79828\n",
      "\n",
      "Ergebnisse bei 60000 Features\n",
      "Accuracy: 79.76%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7960    0.8007    0.7984     39951\n",
      "           1     0.7992    0.7944    0.7968     39877\n",
      "\n",
      "    accuracy                         0.7976     79828\n",
      "   macro avg     0.7976    0.7976    0.7976     79828\n",
      "weighted avg     0.7976    0.7976    0.7976     79828\n",
      "\n",
      "Ergebnisse bei 70000 Features\n",
      "Accuracy: 79.87%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7972    0.8018    0.7995     39951\n",
      "           1     0.8003    0.7957    0.7980     39877\n",
      "\n",
      "    accuracy                         0.7987     79828\n",
      "   macro avg     0.7987    0.7987    0.7987     79828\n",
      "weighted avg     0.7987    0.7987    0.7987     79828\n",
      "\n",
      "Ergebnisse bei 80000 Features\n",
      "Accuracy: 79.92%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7979    0.8019    0.7999     39951\n",
      "           1     0.8005    0.7966    0.7985     39877\n",
      "\n",
      "    accuracy                         0.7992     79828\n",
      "   macro avg     0.7992    0.7992    0.7992     79828\n",
      "weighted avg     0.7992    0.7992    0.7992     79828\n",
      "\n",
      "Ergebnisse bei 90000 Features\n",
      "Accuracy: 79.95%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7984    0.8019    0.8001     39951\n",
      "           1     0.8006    0.7972    0.7989     39877\n",
      "\n",
      "    accuracy                         0.7995     79828\n",
      "   macro avg     0.7995    0.7995    0.7995     79828\n",
      "weighted avg     0.7995    0.7995    0.7995     79828\n",
      "\n",
      "Ergebnisse bei 100000 Features\n",
      "Accuracy: 80.07%\n",
      "--------------------------------------------------------------------------------\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7996    0.8031    0.8013     39951\n",
      "           1     0.8018    0.7983    0.8001     39877\n",
      "\n",
      "    accuracy                         0.8007     79828\n",
      "   macro avg     0.8007    0.8007    0.8007     79828\n",
      "weighted avg     0.8007    0.8007    0.8007     79828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_result_3gram = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
